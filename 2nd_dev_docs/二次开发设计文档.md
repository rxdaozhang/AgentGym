
# AgentGym二次开发设计文档（更新版）

## 一、项目扩展需求概述

本文档详细描述如何扩展AgentGym框架以支持以下功能：

1. 新增数据集支持：GAIA、Webshop、Agenttuning、agentcompany
2. 服务器端并行请求处理
3. 并发roll out和训练服务
4. 灵活替换RL训练框架

## 二、新增数据集支持方案

### 2.1 数据集环境服务器实现

#### 2.1.1 目录结构

```
AgentGym/
├── agentenv-gaia/             # GAIA环境服务器实现
├── agentenv-agenttuning/      # Agenttuning环境服务器实现
├── agentenv-agentcompany/     # agentcompany环境服务器实现
└── agentenv-webshop/          # 已存在，可能需要扩展
```

#### 2.1.2 环境类实现

每个环境服务器需先实现一个基础环境类，作为实际的环境逻辑处理单元：

```python
# 以GAIA为例
class GAIAEnv:
    def __init__(self, params):
        """初始化GAIA环境"""
        self.params = params
        self.dataset = self._load_dataset(params.get("dataset_path"))
        self.current_idx = None
        self.current_state = None
        self.available_actions = []
        self.history = []
        
    def _load_dataset(self, dataset_path):
        """加载数据集"""
        # 实现数据集加载逻辑
        pass
        
    def get_observation(self):
        """获取当前环境状态的观察"""
        return {
            "observation": self.current_state,
            "available_actions": self.available_actions
        }
        
    def get_available_actions(self):
        """获取可用动作列表"""
        return {"available_actions": self.available_actions}
        
    def step(self, action):
        """执行动作并返回结果"""
        # 根据动作更新环境状态
        # 计算奖励
        # 判断是否完成
        reward = self._calculate_reward(action)
        done = self._check_done()
        self.history.append({"action": action, "state": self.current_state})
        
        return {
            "observation": self.current_state,
            "reward": reward,
            "done": done,
            "info": {"action_count": len(self.history)}
        }
        
    def reset(self, idx=None):
        """重置环境到指定索引的任务"""
        if idx is None and self.current_idx is not None:
            idx = self.current_idx
        elif idx is None:
            idx = 0
            
        self.current_idx = idx
        self.current_state = self._get_initial_state(idx)
        self.available_actions = self._get_initial_actions(idx)
        self.history = []
        
        return self.get_observation()
        
    def _calculate_reward(self, action):
        """计算执行动作后的奖励"""
        # 实现奖励计算逻辑
        pass
        
    def _check_done(self):
        """检查任务是否完成"""
        # 实现任务完成检查逻辑
        pass
        
    def _get_initial_state(self, idx):
        """获取指定索引任务的初始状态"""
        # 实现初始状态获取逻辑
        return self.dataset[idx]["initial_state"]
        
    def _get_initial_actions(self, idx):
        """获取指定索引任务的初始可用动作"""
        # 实现初始可用动作获取逻辑
        return self.dataset[idx].get("initial_actions", [])
```

#### 2.1.3 环境服务器实现原型

每个新环境需实现以下服务器类，调用上述环境类：

```python
# 以GAIA为例
class GAIAEnvServer:
    def __init__(self, config):
        self.config = config
        self.env_instances = {}
        self.env_locks = {}  # 用于并发访问控制

    def create_env(self, env_id, params):
        """创建环境实例"""
        self.env_instances[env_id] = GAIAEnv(params)  # 使用GAIAEnv类
        self.env_locks[env_id] = threading.Lock()  # 为每个环境创建一个锁
        return {"env_id": env_id, "status": "created"}

    def get_observation(self, env_id):
        """获取当前环境观察"""
        with self.env_locks[env_id]:
            return self.env_instances[env_id].get_observation()

    def get_available_actions(self, env_id):
        """获取可用动作"""
        with self.env_locks[env_id]:
            return self.env_instances[env_id].get_available_actions()

    def step(self, env_id, action):
        """执行动作"""
        with self.env_locks[env_id]:
            return self.env_instances[env_id].step(action)

    def reset(self, env_id, params=None):
        """重置环境"""
        with self.env_locks[env_id]:
            return self.env_instances[env_id].reset(params.get("idx"))
```

### 2.2 环境客户端实现

在`agentenv/agentenv/envs/`目录下实现对应的客户端类：

```python
# 以GAIA为例
class GAIAEnvClient(BaseEnvClient):
    conversation_start = ("我是GAIA环境助手，请告诉我你想要做什么？",)

    def __init__(self, server_url, dataset_path=None):
        super().__init__()
        self.server_url = server_url
        self.dataset = self._load_dataset(dataset_path)
        self.current_state = None
        self.env_id = None
        self.session = requests.Session()  # 使用会话提高HTTP连接效率

    def __len__(self) -> int:
        return len(self.dataset)

    def observe(self) -> str:
        """获取环境观察"""
        response = self.session.get(f"{self.server_url}/observation", 
                                params={"env_id": self.env_id})
        return response.json()["observation"]

    def step(self, action) -> StepOutput:
        """执行动作并获取结果"""
        response = self.session.post(f"{self.server_url}/step",
                                json={"env_id": self.env_id, "action": action})
        result = response.json()
        return StepOutput(
            observation=result["observation"],
            reward=result.get("reward", 0),
            done=result.get("done", False),
            info=result.get("info", {})
        )

    def reset(self, idx: int) -> None:
        """重置环境到指定索引"""
        if self.env_id is None:
            response = self.session.post(f"{self.server_url}/createEnv")
            self.env_id = response.json()["env_id"]

        self.session.post(f"{self.server_url}/reset",
                     json={"env_id": self.env_id, "idx": idx})
        self.current_state = self.observe()
```

### 2.3 任务类实现

```python
class GAIATask(BaseTask):
    env_client_cls = GAIAEnvClient
    env_name = "gaia"

    def __init__(self, client_args, n_clients=1):
        super().__init__(client_args, n_clients)

    def _tokenize_conversation_one(self, conversation):
        # 处理GAIA特定的对话格式
        return super()._tokenize_conversation_one(conversation)
```

## 三、服务器端并行请求支持

### 3.1 并发HTTP服务器设计

使用Flask或FastAPI实现并发HTTP服务器，通过线程池和进程池实现并发处理：

```python
# 环境服务器基类
from flask import Flask, request, jsonify
import uuid
from concurrent.futures import ThreadPoolExecutor

class BaseEnvServer:
    def __init__(self, max_concurrent_envs=100, max_workers=20):
        self.env_instances = {}
        self.env_locks = {}
        self.max_concurrent_envs = max_concurrent_envs
        self.executor = ThreadPoolExecutor(max_workers=max_workers)  # 线程池处理请求

    def create_env(self, env_id):
        """创建环境"""
        if len(self.env_instances) >= self.max_concurrent_envs:
            raise Exception("达到最大并发环境数量限制")

        self.env_instances[env_id] = self._create_env_instance()
        self.env_locks[env_id] = threading.Lock()
        return {"env_id": env_id, "status": "created"}

    def step(self, env_id, action):
        """执行动作"""
        with self.env_locks[env_id]:
            return self._step(self.env_instances[env_id], action)

    def _step(self, env_instance, action):
        """实际的步骤执行"""
        raise NotImplementedError()
```

### 3.2 HTTP服务实现（基于Flask）

```python
# 以GAIA为例
app = Flask(__name__)
server = GAIAEnvServer()

@app.route("/createEnv", methods=["POST"])
def create_env():
    env_id = str(uuid.uuid4())
    # 提交任务到线程池处理
    future = server.executor.submit(server.create_env, env_id)
    result = future.result()
    return jsonify(result)

@app.route("/observation", methods=["GET"])
def get_observation():
    env_id = request.args.get("env_id")
    if env_id not in server.env_instances:
        return jsonify({"error": "Environment not found"}), 404
    # 提交任务到线程池处理
    future = server.executor.submit(server.get_observation, env_id)
    result = future.result()
    return jsonify(result)

@app.route("/step", methods=["POST"])
def step():
    data = request.json
    env_id = data.get("env_id")
    action = data.get("action")
    if env_id not in server.env_instances:
        return jsonify({"error": "Environment not found"}), 404
    # 提交任务到线程池处理
    future = server.executor.submit(server.step, env_id, action)
    result = future.result()
    return jsonify(result)

@app.route("/reset", methods=["POST"])
def reset():
    data = request.json
    env_id = data.get("env_id")
    idx = data.get("idx")
    if env_id not in server.env_instances:
        return jsonify({"error": "Environment not found"}), 404
    # 提交任务到线程池处理
    future = server.executor.submit(server.reset, env_id, idx)
    result = future.result()
    return jsonify(result)

if __name__ == "__main__":
    # 使用多线程模式运行应用
    app.run(host="0.0.0.0", port=8000, threaded=True)
```

## 四、并发Roll Out和训练服务

### 4.1 数据库设计

使用MongoDB存储轨迹数据：

```python
# 数据模型
class TrajectoryModel:
    def __init__(self):
        self.client = pymongo.MongoClient("mongodb://localhost:27017/")
        self.db = self.client["agentgym"]
        self.trajectories = self.db["trajectories"]

    def save_trajectory(self, traj_data):
        """保存轨迹到数据库"""
        doc = {
            "traj_id": str(uuid.uuid4()),
            "env_name": traj_data["env_name"],
            "task_id": traj_data["task_id"],
            "agent_id": traj_data["agent_id"],
            "timestamp": datetime.now(),
            "trajectory": traj_data["trajectory"],
            "metrics": traj_data.get("metrics", {})
        }
        self.trajectories.insert_one(doc)
        return doc["traj_id"]

    def get_trajectories(self, query=None, limit=100):
        """获取轨迹数据"""
        if query is None:
            query = {}
        cursor = self.trajectories.find(query).limit(limit)
        return list(cursor)
```

### 4.2 并发Roll Out控制器

使用线程池实现并发处理：

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

class ConcurrentRolloutController(BaseAgentEnvController):
    def __init__(self, agent, task, db_model=None, max_workers=10):
        super().__init__(agent, task)
        self.db_model = db_model or TrajectoryModel()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    def rollout(self, generation_config, max_rounds, idxs):
        """并发执行roll out"""
        futures = {}
        results = []

        # 提交任务到线程池
        for idx in idxs:
            future = self.executor.submit(
                self._rollout_one, idx, generation_config, max_rounds
            )
            futures[future] = idx

        # 收集结果
        for future in as_completed(futures):
            idx = futures[future]
            try:
                exp_output = future.result()
                
                # 保存到数据库
                traj_data = {
                    "env_name": self.task.env_name,
                    "task_id": idx,
                    "agent_id": getattr(self.agent, "id", "unknown"),
                    "trajectory": exp_output.to_dict(),
                }
                self.db_model.save_trajectory(traj_data)
                
                results.append(exp_output)
            except Exception as e:
                print(f"任务 {idx} 执行失败: {e}")
        
        return results

    def _rollout_one(self, idx, generation_config, max_rounds):
        """执行单个环境的roll out"""
        self.task.reset(idx)
        conversation = list(self.task.conversation_start)
        
        for _ in range(max_rounds):
            # 获取观察
            observation = self.task.observe()
            conversation.append(observation)

            # 生成动作
            inputs = self.task._tokenize_conversation(conversation)
            outputs = self.agent.model.generate(**inputs, **generation_config)
            action = self.agent.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # 执行动作
            step_output = self.task.step(action)
            conversation.append(action)

            if step_output.done:
                break

        return ExperienceOutput(
            conversation=conversation,
            reward=step_output.reward,
            done=step_output.done,
            info=step_output.info
        )
```

### 4.3 并发训练器

```python
class ConcurrentTrainer(BaseTrainer):
    def __init__(self, agent, task, db_model=None, max_workers=5, **kwargs):
        super().__init__(agent, task, **kwargs)
        self.db_model = db_model or TrajectoryModel()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    def train(self, batch_size=16, learning_rate=2e-5, epochs=3):
        """训练方法"""
        # 从数据库获取轨迹
        trajectories = self.db_model.get_trajectories(
            query={"env_name": self.task.env_name},
            limit=1000
        )

        # 准备训练数据
        train_data = self._prepare_training_data(trajectories)

        # 训练模型
        optimizer = torch.optim.AdamW(self.agent.model.parameters(), lr=learning_rate)
        self.agent.model.train()

        for epoch in range(epochs):
            # 并行处理批次
            batch_futures = []
            for batch in self._get_batches(train_data, batch_size):
                future = self.executor.submit(self._process_batch, batch, optimizer)
                batch_futures.append(future)

            # 收集批次结果
            total_loss = 0
            for future in as_completed(batch_futures):
                batch_loss = future.result()
                total_loss += batch_loss

            print(f"Epoch {epoch+1}, Loss: {total_loss}")

        return {"status": "complete", "epochs": epochs, "final_loss": total_loss}
    
    def _process_batch(self, batch, optimizer):
        """并行处理单个批次"""
        optimizer.zero_grad()
        outputs = self.agent.model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        return loss.item()
```

## 五、Controller模块扩展设计

### 5.1 Controller继承关系设计

以下是二次开发中各组件与Controller模块的继承关系：

1. **环境客户端类继承关系**:
   ```
   BaseEnvClient (controller/env.py)
   ├── GAIAEnvClient
   ├── AgenttuningEnvClient
   ├── AgentcompanyEnvClient
   └── 扩展后的WebshopEnvClient
   ```

2. **任务类继承关系**:
   ```
   BaseTask (controller/task.py)
   ├── GAIATask
   ├── AgenttuningTask
   ├── AgentcompanyTask
   └── 扩展后的WebshopTask
   ```

3. **控制器类继承关系**:
   ```
   BaseAgentEnvController (controller/utils.py)
   ├── Evaluator (controller/utils.py)
   │   └── ConcurrentEvaluator
   ├── BaseTrainer (controller/utils.py)
   │   ├── ConcurrentTrainer
   │   └── PluggableRLTrainer
   └── ConcurrentRolloutController
   ```

### 5.2 接口对齐设计

为确保新组件与Controller模块接口的一致性，需要实现以下接口映射：

#### 5.2.1 并发环境客户端增强

环境客户端可增加线程安全的并发处理：

```python
class ThreadSafeEnvClient(BaseEnvClient):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.lock = threading.Lock()
        
    def observe(self) -> str:
        """线程安全的观察方法"""
        with self.lock:
            return super().observe()
        
    def step(self, action) -> StepOutput:
        """线程安全的步骤方法"""
        with self.lock:
            return super().step(action)
        
    def reset(self, idx: int) -> None:
        """线程安全的重置方法"""
        with self.lock:
            super().reset(idx)
```

#### 5.2.2 并发任务类增强

任务类可以增加对并发操作的支持：

```python
class ConcurrentBaseTask(BaseTask):
    def generate_experience_concurrent(self, model, tokenizer, idxs, generation_config, max_rounds, max_workers=10) -> list[ExperienceOutput]:
        """并发生成经验"""
        executor = ThreadPoolExecutor(max_workers=max_workers)
        futures = []
        
        for idx in idxs:
            future = executor.submit(
                self._generate_experience_one, 
                model, tokenizer, idx, generation_config, max_rounds
            )
            futures.append(future)
        
        results = []
        for future in as_completed(futures):
            results.append(future.result())
            
        return results
        
    def _generate_experience_one(self, model, tokenizer, idx, generation_config, max_rounds):
        """生成单个任务的经验"""
        # 具体实现...
```

#### 5.2.3 ConcurrentEvaluator接口对齐

```python
class ConcurrentEvaluator(Evaluator):
    def __init__(self, agent, tasks, max_workers=10):
        super().__init__(agent, tasks)
        self.max_workers = max_workers
        
    def eval(self, generation_config, max_rounds, idxs) -> EvaluationOutput:
        """并发评估智能体"""
        executor = ThreadPoolExecutor(max_workers=self.max_workers)
        futures = {}
        
        for task_idx, task in enumerate(self.tasks):
            # 为每个任务提交并发评估
            future = executor.submit(
                task.generate_experience_concurrent,
                self.agent.model, 
                self.agent.tokenizer, 
                idxs, 
                generation_config, 
                max_rounds,
                self.max_workers
            )
            futures[future] = task_idx
            
        # 收集结果
        task_outputs = [None] * len(self.tasks)
        for future in as_completed(futures):
            task_idx = futures[future]
            task_outputs[task_idx] = future.result()
            
        # 处理评估结果
        # ...其余处理与原Evaluator相同
```

### 5.3 数据结构对齐

继续使用与controller模块兼容的数据类型：

1. **ConversationMessage**: 同现有格式存储对话
2. **ExperienceOutput**: 同现有格式存储经验数据
3. **EvaluationOutput**: 同现有格式存储评估结果

需确保数据结构的线程安全性，特别是在并发操作中。

## 六、RL训练框架替换支持

### 6.1 抽象RL框架接口

```python
class RLFramework(metaclass=ABCMeta):
    @abstractmethod
    def train(self, agent, env, config):
        """训练方法"""
        pass

    @abstractmethod
    def evaluate(self, agent, env, config):
        """评估方法"""
        pass

    @abstractmethod
    def save_model(self, agent, path):
        """保存模型"""
        pass

    @abstractmethod
    def load_model(self, agent, path):
        """加载模型"""
        pass
```

### 6.2 VERL框架适配器

```python
class VERLAdapter(RLFramework):
    def __init__(self, verl_config=None):
        self.verl_config = verl_config or {}

    def train(self, agent, env, config):
        """使用VERL框架训练智能体"""
        import verl

        # 转换AgentGym的环境为VERL可接受的环境
        verl_env = self._convert_to_verl_env(env)

        # 创建VERL训练器
        trainer = verl.PPOTrainer(
            policy=self._wrap_agent_as_policy(agent),
            env=verl_env,
            **self.verl_config
        )

        # 执行训练
        results = trainer.train(**config)
        return results

    def _convert_to_verl_env(self, env):
        """将AgentGym环境转换为VERL环境"""
        # 实现环境转换逻辑
        pass

    def _wrap_agent_as_policy(self, agent):
        """将AgentGym智能体包装为VERL策略"""
        # 实现策略包装逻辑
        pass
```

### 6.3 RL训练器基类扩展

```python
class PluggableRLTrainer(BaseTrainer):
    def __init__(self, agent, task, rl_framework, **kwargs):
        super().__init__(agent, task, **kwargs)
        self.rl_framework = rl_framework

    def train(self, rl_config=None):
        """使用可插拔RL框架训练智能体"""
        rl_config = rl_config or {}
        return self.rl_framework.train(self.agent, self.task, rl_config)

    def eval(self, generation_config, max_rounds, idxs):
        """使用可插拔RL框架评估智能体"""
        eval_config = {
            "generation_config": generation_config,
            "max_rounds": max_rounds,
            "idxs": idxs
        }
        return self.rl_framework.evaluate(self.agent, self.task, eval_config)
```

## 七、技术挑战与解决方案

1. **环境复杂性**：不同环境具有不同特性
   - 解决方案：设计灵活的接口，允许环境特定的扩展

2. **并发性能**：并发处理大量请求可能导致资源竞争
   - 解决方案：使用线程池、连接池和锁机制管理资源

3. **数据一致性**：并发操作可能导致数据不一致
   - 解决方案：使用锁机制和事务确保数据一致性

4. **框架兼容性**：不同RL框架有不同的接口
   - 解决方案：设计抽象适配器模式，隔离框架差异

5. **HTTP连接效率**：大量HTTP请求可能导致连接瓶颈
   - 解决方案：使用连接池，保持长连接，减少连接建立开销

## 八、总结

本文档详细描述了AgentGym框架的二次开发设计，包括新增数据集支持、服务器端并行请求处理、并发roll out和训练服务以及灵活的RL训练框架替换。通过这些扩展，AgentGym将能够支持更多环境、提高并发性能并增强系统的可扩展性，同时保持与现有架构的一致性和兼容性。
